<iframe width="621" height="480" src="https://www.youtube.com/embed/QBI9nLX1ZzE" title="Giuseppe Longo: Alphabets, Axioms, DNA: On Human Knowledge and the Myth of Alphanumeric Coding" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>


- Non linear systems were born out of the fact that linguistic axiomatics are not logically complete
- Laplacian - means that everything in the program is completely determined
- Randomness is not outside determination
- Randomness and Necessity: Randomness is noise and outside determinations, that isn't true
- [[Hilbert's Program]]: Axiomatic treatment of mathematics
- Is DNA code an alphabetic program? A linguistic program of Biology
- Is DNA a logically complete system?
- Diversity is due to noise - that's more politics than science
- Schrodinger's discussion: "code-script" of a chromosome. Is this completely determined system
- Venter: We understood nothing from decoding the human genome
- Central Dogma of molecular biology: The DNA code is information complete. Is it?
- Genes for marital infidelity, and longevity, seduction
- Incompleteness does not mean useless
- What does it mean to be incomplete
- Exact Stereospecificity of macro molecules of proteins: Conjecture: Key Lock paradigm
- Sufficiently different and the Key Lock paradigm
- Asbestos causes a cut in the cell inter locking material
- 100,000 artificial molecules, and their negative impact on the evolution of the human race
- Linguistification of DNA causes GMO's
- The ground is modified, due to GMO's
- Isomorphism between one gene and one protein. This is suspect. 
- Move from central dogma to closure constraints

# Expanded Notes

1. **Non-linear Systems and Linguistic Axiomatics**: Non-linear systems in mathematics and science often exhibit complex, unpredictable behaviors that cannot be fully described by traditional linear axioms. This relates to the idea that linguistic structures (used in logic and mathematics) may not be sufficient to capture the full complexity of natural phenomena.

2. **Laplacian Determinism**: Refers to the philosophical idea that if someone (a hypothetical "Laplace's Demon") knew the precise location and momentum of every atom in the universe, the future and past could be perfectly predicted. This contrasts with the concept of randomness and suggests a universe completely determined by initial conditions.

3. **Randomness and Determination**: The idea here challenges the traditional view of randomness as entirely outside of deterministic systems. It suggests that what appears as random may still be a part of a larger deterministic framework.

4. **Hilbert's Program**: This was an early 20th-century initiative in mathematics aiming to provide a complete and consistent set of axioms for all mathematics. It was significantly impacted by Gödel's incompleteness theorems.

5. **DNA as a Linguistic Program**: This analogy likens the genetic code to a language or programming system. It raises questions about whether DNA, like a linguistic system, might be incomplete or subject to limitations in expressing biological complexity.

6. **Diversity and Noise**: This note suggests a political interpretation of biological diversity, possibly critiquing views that attribute diversity solely to genetic "noise" or randomness rather than deterministic processes.

7. **Schrodinger's "Code-Script"**: Refers to Erwin Schrödinger's idea that chromosomes contain a "code-script" for an organism's development. The question here is whether this script is fully deterministic.

8. **Venter on the Human Genome**: Craig Venter, a prominent figure in genomics, noted that decoding the human genome did not lead to a complete understanding of human biology, suggesting the complexity and potential incompleteness of genetic information.

9. **Central Dogma of Molecular Biology**: This principle posits that information flows from DNA to RNA to protein but not in reverse. The question raised here is about the completeness and exclusivity of this information flow.

10. **Genes for Complex Traits**: Refers to the identification of genes associated with complex human behaviors and traits, such as marital infidelity or longevity.

11. **Incompleteness in Science**: Recognizes that incomplete systems or theories can still be useful and valuable in scientific understanding.

12. **Exact Stereospecificity of Macromolecules**: This involves the idea that molecular interactions, like enzyme-substrate interactions, are highly specific ("key-lock paradigm"). This specificity is fundamental to biological processes.

13. **Impact of Artificial Molecules**: This point raises concerns about how synthetic molecules, like drugs or pollutants, might affect human evolution and health.

14. **Linguistification of DNA and GMOs**: Discusses the concept of genetically modified organisms (GMOs) as a form of applying linguistic (programming) principles to biology, altering the genetic "language."

15. **Environmental Impact of GMOs**: Addresses the broader ecological and environmental implications of genetically modified organisms.

16. **Gene-Protein Isomorphism**: This is about the relationship between genes and the proteins they encode. The note suggests skepticism about a one-to-one correspondence between a single gene and a single protein.

17. **From Central Dogma to Closure Constraints**: Suggests a shift in thinking from a linear flow of genetic information to a more complex system of constraints and interactions within biological systems.



# Idea Map

1. **Non-linear Systems and Linguistic Axiomatics**
    
    - Mathematical Complexity
        - Complex Dynamics
            - Chaos Theory
        - Predictive Limitations
            - Uncertainty in Modeling
    - Linguistic Structures
        - Logic and Mathematics
            - Formal Language Theory
        - Inadequacy for Natural Phenomena
            - Limits of Descriptive Language
2. **Laplacian Determinism**
    
    - Concept Overview
        - Determinism in Philosophy
            - Cause and Effect
        - Laplace's Demon Hypothesis
            - Theoretical Implications
    - Contrasts
        - Randomness
            - Probabilistic Systems
        - Quantum Mechanics
            - Uncertainty Principle
3. **Randomness and Determination**
    
    - Nature of Randomness
        - Statistical Randomness
            - Probability Theory
        - Perceived Randomness
            - Human Perception
    - Deterministic Frameworks
        - Systemic Determinism
            - Causal Networks
        - Illusion of Randomness
            - Hidden Variables
4. **Hilbert's Program**
    
    - Historical Context
        - Early 20th Century Mathematics
            - Foundations of Mathematics
        - Axiomatic Systems
            - Formalism
    - Gödel's Impact
        - Incompleteness Theorems
            - Mathematical Limitations
        - Philosophical Implications
            - Epistemological Questions
5. **DNA as a Linguistic Program**
    
    - Genetic Code
        - Biological Information Encoding
            - Molecular Genetics
        - Comparison with Language
            - Syntax and Semantics
    - Limitations and Incompleteness
        - Genetic Determinism
            - Nature vs. Nurture
        - Epigenetic Factors
            - Non-Genetic Influences
6. **Diversity and Noise**
    
    - Biological Diversity
        - Genetic Variation
            - Evolutionary Biology
        - Species Adaptation
            - Ecological Dynamics
    - Political Interpretations
        - Genetic "Noise"
            - Random Mutations
        - Determinism in Diversity
            - Environmental Influences
7. **Schrodinger's "Code-Script"**
    
    - Chromosomal Information
        - Genetic Blueprint
            - Developmental Biology
        - Deterministic Interpretation
            - Genetic Predeterminism
    - Philosophical Questions
        - Free Will and Determinism
            - Ethical Implications
        - Scientific Limitations
            - Predictive Constraints

# Laplacian Demon Hypothesis

Laplace's Demon is a thought experiment first articulated by Pierre-Simon Laplace in the early 19th century. It presents a hypothetical being capable of knowing the precise location and momentum of every atom in the universe at a given moment. Based on this information, Laplace proposed that the demon would be able to use the laws of classical mechanics to predict the future of every particle, and thus the entire universe, with perfect accuracy. This concept is often used to illustrate the ideas of determinism and predictability in a Newtonian universe.

Key aspects of Laplace's Demon hypothesis include:

1. **Determinism**: The hypothesis is rooted in a deterministic view of the universe, where the future state of the universe is a direct and predictable consequence of its current state.

2. **Classical Mechanics**: It relies on the principles of Newtonian physics, which were seen at the time as capable of describing all physical phenomena.

3. **Omniscience**: The demon, as a theoretical construct, possesses complete knowledge of the properties (position, velocity, etc.) of every particle in the universe.

4. **Predictability**: Given its comprehensive knowledge, the demon could predict all future events, suggesting that the universe is fundamentally predictable.

5. **Philosophical Implications**: This idea raises questions about free will, causality, and the nature of reality, challenging the notion that the universe contains random or unpredictable elements.

However, with the advent of quantum mechanics in the 20th century, the validity of Laplace's Demon as a concept was fundamentally challenged. Quantum mechanics introduced inherent uncertainties in the state of particles (as famously encapsulated in Heisenberg's Uncertainty Principle), which imply that it is impossible to know both the position and momentum of a particle with perfect precision. This introduces an element of fundamental unpredictability at the quantum level, which stands in contrast to the entirely predictable universe imagined by Laplace's Demon.

# Epistemological Questions around Godel's Incompleteness theorem 

Gödel's incompleteness theorems, formulated by Kurt Gödel in the 1930s, have profound implications for epistemology, the branch of philosophy concerned with the nature and scope of knowledge. These theorems have sparked various epistemological questions and discussions, particularly around the limits of knowledge, the nature of mathematical truths, and the capabilities of formal systems. Here are some key epistemological questions and considerations arising from Gödel's theorems:

1. **Limits of Formal Systems**: Gödel's first incompleteness theorem states that any consistent formal system capable of expressing basic arithmetic is incomplete; that is, there are true statements in the system that cannot be proved within the system. This raises epistemological questions about the limits of what can be known or proven within a given logical or mathematical system.

2. **Nature of Mathematical Truth**: The existence of true but unprovable statements within a formal system challenges the traditional correspondence theory of truth, which holds that truth corresponds to provability in a formal system. Gödel's theorems suggest that mathematical truth might transcend formal proof, leading to inquiries about the nature and source of mathematical truths.

3. **Scope of Human Knowledge**: Gödel's work implies that human understanding and knowledge, particularly in mathematics, is inherently limited. This limitation raises questions about the potential and boundaries of human reason and whether there exist truths that are inherently beyond human comprehension.

4. **Platonism in Mathematics**: The theorems are often seen as supporting mathematical Platonism, the view that mathematical entities exist independently of human minds. The existence of objective mathematical truths that are unprovable within any formal system suggests that these truths exist in some abstract realm, independent of human constructs.

5. **Computability and Logic**: Gödel's work, alongside Alan Turing's work on computability, raises questions about the relationship between logic, computation, and knowledge. This includes exploring the capabilities and limits of algorithmic knowledge and whether all logical truths are computable.

6. **Philosophy of Mind**: The theorems have implications for theories of mind and cognition, particularly in discussions about whether the human mind can be fully represented or replicated by a formal system or computer program. Gödel's theorems suggest there might be aspects of human thought that transcend formalizable logic.

7. **Implications for Scientific Theories**: In science, the pursuit of a complete and consistent set of axioms to explain the universe (akin to Hilbert's program in mathematics) is questioned by Gödel's results. It raises the possibility that no single theoretical framework can ever fully encapsulate all physical truths.

8. **The Consistency of Mathematics**: Gödel's second incompleteness theorem states that a system cannot demonstrate its own consistency. This leads to epistemological inquiries about the foundations of mathematics and whether and how the consistency of foundational mathematical systems can be assured.

These questions highlight the deep and enduring impact of Gödel's theorems on philosophical thought, particularly in the realms of logic, mathematics, and the theory of knowledge. They underscore the profound complexity and mystery at the heart of epistemology and the pursuit of understanding in the realms of abstract thought.

# Notes: 
This is interesting because in the Library of Babel theory of Scientific theory, and knowledge production, because of incompleteness we are unable to go through entire planes of defined knowledge within the entire map of knowledge. Imagination actually can traverse these fields can it not?